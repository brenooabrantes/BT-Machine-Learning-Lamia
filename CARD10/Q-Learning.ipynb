{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "O aprendizado por reforço ajuda a desenvolver a melhor forma para executar o trabalho por tentativa e erro, conforme é explorado se calcula a melhor opção que o agente possui para realizar sua função, que neste caso, o taxi tomar a melhor rota."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\", render_mode='ansi').env #New versions keep getting released; if -v3 doesn't work, try -v2 or -v4\n",
    "streets.reset()\n",
    "print(\"\\n\" + streets.render()) ##printar e renderizar nossas ruas, essa elaboradas no gym.make"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-  R, G, B, e Y são os locais para pegar ou deixar passageiros.\n",
    "-  O azul onde é preciso passar para pegar o passageiro\n",
    "-  A magenta representa onde o passageiro quer chegar\n",
    "-  As linhas retas representam as ruas que ele não pode atravessar\n",
    "-  O retângulo amarelo representa o taxi, quando mantém a cor ele está vazio e verde quando possui um passageiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Lugares acessáveis representam um 5x5 = 25 lugares \n",
    "-  Existem 4 possibilidades de destino\n",
    "- E contando a localização do passageiro se tem 5\n",
    "\n",
    "Nesse caso, 25 x 4 x 5 = 500 possibilidades \n",
    "\n",
    "6 possibilidades de ação:\n",
    "\n",
    "-  Se movimentar para sul, leste, norte ou oeste\n",
    "-  Pegar o passageiro\n",
    "-  Deixá-lo\n",
    "\n",
    "O Q-Learning dará as seguintes recompensas e penalidades de cada forma:\n",
    "\n",
    "Deixar o passageiro de forma correta concede +20 pontos\n",
    "Cada unidade de tempo enquanto dirige com um passageiro gera uma penalidade de -1 ponto\n",
    "Pegar ou deixar em um local ilegal gera uma penalidade de -10 pontos\n",
    "\n",
    "Vamos definir um estado inicial, com o taxi na loc (2, 3), com o passageiro a ser pego na 2 e ser deixado na 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "initial_state = streets.encode(2, 3, 2, 0) ##aqui o código explicado acima\n",
    "\n",
    "streets.s = initial_state ##variavel para guardar os dados\n",
    "\n",
    "streets.render() #renderizar "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "streets.P[initial_state] ##printando o estado inicial definido anteriormente e verificando as locs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mover para o Sul, Norte, Leste ou Oeste, pegar passageiro ou desembarcar. Os quatro valores em cada linha representam a probabilidade atribuída a essa ação, o maior foco é entregar o passageiro ao seu destino\n",
    "\n",
    "Por exemplo, mover-se para o Norte a partir deste estado nos colocaria no estado número 368, acarretaria uma penalidade de -1 por ocupar tempo e não resultar em um desembarque correto.\n",
    "\n",
    "Primeiro, treinaremos o modelo. Em um nível geral, faremos o treinamento ao longo de 10mil corridas de táxi. Em cada corrida, avançaremos no tempo, com uma chance de 10% em cada passo de tomar uma ação aleatória em vez de usar os valores Q aprendidos para orientar o agente."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np ##import biblioteca para lidar com os numeros\n",
    "\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n]) ##criar tabela\n",
    "\n",
    "learning_rate = 0.1 ##valor atribuido à \"importancia\" de cada teste e quanto isso soma pro  aprendizado\n",
    "discount_factor = 0.6 \n",
    "exploration = 0.1 ##porcentagem de exploracao sendo 10%\n",
    "epochs = 10000 ##numero de testes para contabilizar o Qlearning\n",
    "\n",
    "for taxi_run in range(epochs): ##iteração pros testes\n",
    "    state, _ = streets.reset() #para pegar apenas o estado inicial, sem isso daria erro pela versao\n",
    "    done = False\n",
    "    truncated = False ##para calcular corretamente quando o agente acabou de fato seu aprendizado\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        random_value = random.uniform(0, 1) ##variança de 0 a 1 para uso aleatorio ou qlearning\n",
    "        if (random_value < exploration):\n",
    "            action = streets.action_space.sample()  ##random\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) ##qlearning\n",
    "            \n",
    "        next_state, reward, done, truncated, info = streets.step(action) ##estado, recompensa, se finalizou ou parou e info adicionais\n",
    "        \n",
    "        prev_q = q_table[state, action] ##atualizacao de valores e substituição dos antigos valores nos códigos abaixo\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        q_table[state, action] = new_q\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "q_table[initial_state]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from time import sleep\n",
    "\n",
    "for tripnum in range(1, 11): ##10 viagens\n",
    "    state, _ = streets.reset() ##zerar o ambiente\n",
    "    done = False\n",
    "    truncated = False\n",
    "    trip_length = 0 ##duração\n",
    "    \n",
    "    while not done and truncated and trip_length < 25: ##para cada viagem não ter passos demais e o teste perder o sentido \n",
    "        \n",
    "        action = np.argmax(q_table[state]) ##buscar ação\n",
    "        next_state, reward, done, truncated, info = streets.step(action) \n",
    "        \n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "        print(streets.render()) ##printar a execução do taxi\n",
    "        sleep(.5) ##tempo de espera\n",
    "        state, _ = next_state\n",
    "        trip_length += 1 ##incrementador\n",
    "        \n",
    "    sleep(2)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Challenge\n",
    "\n",
    "Modify the block above to keep track of the total time steps, and use that as a metric as to how good our Q-learning system is. You might want to increase the number of simulated trips, and remove the sleep() calls to allow you to run over more samples.\n",
    "\n",
    "Now, try experimenting with the hyperparameters. How low can the number of epochs go before our model starts to suffer? Can you come up with better learning rates, discount factors, or exploration factors to make the training more efficient? The exploration vs. exploitation rate in particular is interesting to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\", render_mode='ansi').env\n",
    "streets.reset()\n",
    "print(\"\\n\" + streets.render())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "learning_rate = 0.1 \n",
    "discount_factor = 0.6  \n",
    "exploration = 0.1 \n",
    "epochs = 10000  "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for taxi_run in range(epochs):\n",
    "    state, _ = streets.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "\n",
    "    while not (done or truncated):\n",
    "        random_value = random.uniform(0, 1)\n",
    "        if random_value < exploration:\n",
    "            action = streets.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        next_state, reward, done, truncated, info = streets.step(action)\n",
    "\n",
    "        prev_q = q_table[state, action]\n",
    "        next_max_q = np.max(q_table[next_state])\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "        q_table[state, action] = new_q\n",
    "\n",
    "        state = next_state\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_time_steps = 0 #variavel para cada passo do taxi\n",
    "num_trips = 100  #numero de viagens \n",
    "\n",
    "for tripnum in range(1, num_trips + 1):\n",
    "    state, _ = streets.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    trip_length = 0\n",
    "\n",
    "    while not (done or truncated) and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, truncated, info = streets.step(action)\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "\n",
    "    total_time_steps += trip_length  #acumula cada passo do taxi\n",
    "\n",
    "print(f\"Total time steps in {num_trips} travel: {total_time_steps}\") #printando cada passo por viagem"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vamos criar hiperparâmetros aleatórios para fazer várias verificações diferentes nas viagens, todos esses parâmetros são explicados anteriormente do porquê existirem e sua importânncia."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hyperparameters = [\n",
    "    {'learning_rate': 0.1, 'discount_factor': 0.6, 'exploration': 0.1, 'epochs': 10000},\n",
    "    {'learning_rate': 0.7, 'discount_factor': 0.2, 'exploration': 0.3, 'epochs': 7000},\n",
    "    #{'learning_rate': 0.5, 'discount_factor': 0.9, 'exploration': 0.2, 'epochs': 10000},\n",
    "    ]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#iteração dos parametros sob os listados acima\n",
    "for params in hyperparameters:\n",
    "    learning_rate = params['learning_rate']\n",
    "    discount_factor = params['discount_factor']\n",
    "    exploration = params['exploration']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    q_table = np.zeros([streets.observation_space.n, streets.action_space.n])\n",
    "\n",
    "    for taxi_run in range(epochs):\n",
    "        state, _ = streets.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (done or truncated):\n",
    "            random_value = random.uniform(0, 1)\n",
    "            if random_value < exploration:\n",
    "                action = streets.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            next_state, reward, done, truncated, info = streets.step(action)\n",
    "\n",
    "            prev_q = q_table[state, action]\n",
    "            next_max_q = np.max(q_table[next_state])\n",
    "            new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q)\n",
    "            q_table[state, action] = new_q\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    total_time_steps = 0\n",
    "\n",
    "    for tripnum in range(1, num_trips + 1):\n",
    "        state, _ = streets.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        trip_length = 0\n",
    "\n",
    "        while not (done or truncated) and trip_length < 25:\n",
    "            action = np.argmax(q_table[state])\n",
    "            next_state, reward, done, truncated, info = streets.step(action)\n",
    "            #print(streets.render())  #mostrar o taxi sendo testado\n",
    "            trip_length += 1\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            #sleep(0.1)\n",
    "\n",
    "        total_time_steps += trip_length #iterar os pasos com a distância\n",
    "        \n",
    "\n",
    "    print(f\"Hyperparameters: {params} | Total time steps: {total_time_steps}\") #printar os hiperparametros setados junto dos passos que foram necessários"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ao final da atividade e com inclusão de várias novas etapas é possível retirar algumas conclusões: \n",
    "Utilizar os hiperparametros é muito interessantes para testar as diferentes formas que um programa pode aprender com mudanças no seu modelo de teste, pode-se ver o taxi em movimento e como ele se comportará em cada mapa diferente baseado nos locais de pickoff e dropoff, ou seja, onde pegará e deixará o passageiro, podemos testar o número de viagens, taxa de aprendizado, dados coletados, os passos a cada viagem e cada teste. O Q-learning é muito interessante para o agente criar e otimizar seu trabalho, neste modelo, buscando as melhores rotas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
